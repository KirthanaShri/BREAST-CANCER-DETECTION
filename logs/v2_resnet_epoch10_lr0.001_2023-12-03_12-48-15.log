2023-12-03_12-48-15: config: {'n_epochs': 10, 'batch_size': 32, 'h': 224, 'w': 224, 'lr': 0.001, 'optimizer': 'adam', 'momentum': 0.9, 'device': 'cuda', 'n_gpus': 2, 'kernel_size': 3, 'flatten': False, 'model': 'resnet', 'num_blocks_list': [3, 2, 3, 4], 'is_batchnorm': False}
2023-12-03_12-48-15: conv1.0.weight: torch.Size([64, 3, 3, 3])
2023-12-03_12-48-15: conv2_x.0.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_12-48-15: conv2_x.0.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_12-48-15: conv2_x.1.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_12-48-15: conv2_x.1.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_12-48-15: conv2_x.2.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_12-48-15: conv2_x.2.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_12-48-15: conv3_x.0.sequence.0.weight: torch.Size([128, 64, 3, 3])
2023-12-03_12-48-15: conv3_x.0.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_12-48-15: conv3_x.0.shortcut.0.weight: torch.Size([128, 64, 1, 1])
2023-12-03_12-48-15: conv3_x.1.sequence.0.weight: torch.Size([128, 128, 3, 3])
2023-12-03_12-48-15: conv3_x.1.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_12-48-15: conv4_x.0.sequence.0.weight: torch.Size([256, 128, 3, 3])
2023-12-03_12-48-15: conv4_x.0.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_12-48-15: conv4_x.0.shortcut.0.weight: torch.Size([256, 128, 1, 1])
2023-12-03_12-48-15: conv4_x.1.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_12-48-15: conv4_x.1.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_12-48-15: conv4_x.2.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_12-48-15: conv4_x.2.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_12-48-15: conv5_x.0.sequence.0.weight: torch.Size([512, 256, 3, 3])
2023-12-03_12-48-15: conv5_x.0.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_12-48-15: conv5_x.0.shortcut.0.weight: torch.Size([512, 256, 1, 1])
2023-12-03_12-48-15: conv5_x.1.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_12-48-15: conv5_x.1.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_12-48-15: conv5_x.2.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_12-48-15: conv5_x.2.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_12-48-15: conv5_x.3.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_12-48-15: conv5_x.3.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_12-48-15: fc.weight: torch.Size([2, 512])
2023-12-03_12-48-15: fc.bias: torch.Size([2])
2023-12-03_12-48-15: 
Total parameters: 21,850,818;	Trainable: 21,850,818
2023-12-03_12-49-29: Epoch: 2 | Train loss: 77205.77487847189 | Train acc: {'40X': 65.97, '100X': 67.44, '200X': 65.06, '400X': 65.35, 'avg_acc': 65.96, 'all_acc': 65.98} | Valid loss: 0.6163493978977204 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.2 mins
2023-12-03_12-50-42: Epoch: 3 | Train loss: 0.6172214025581205 | Train acc: {'40X': 68.79, '100X': 69.61, '200X': 68.27, '400X': 68.53, 'avg_acc': 68.8, 'all_acc': 68.81} | Valid loss: 0.6349726748466492 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.2 mins
2023-12-03_12-51-56: Epoch: 4 | Train loss: 0.7800071444865819 | Train acc: {'40X': 67.5, '100X': 67.98, '200X': 67.08, '400X': 66.42, 'avg_acc': 67.24, 'all_acc': 67.27} | Valid loss: 0.6626557064056396 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.2 mins
2023-12-03_12-53-11: Epoch: 5 | Train loss: 0.6613607920102171 | Train acc: {'40X': 67.89, '100X': 69.27, '200X': 67.94, '400X': 67.43, 'avg_acc': 68.13, 'all_acc': 68.16} | Valid loss: 0.6471332478523254 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.2 mins
2023-12-03_12-54-25: Epoch: 6 | Train loss: 0.6328515108372714 | Train acc: {'40X': 68.79, '100X': 69.69, '200X': 68.27, '400X': 68.62, 'avg_acc': 68.84, 'all_acc': 68.86} | Valid loss: 0.8588326328992844 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.2 mins
2023-12-03_12-55-39: Epoch: 7 | Train loss: 0.6289228893615104 | Train acc: {'40X': 68.73, '100X': 69.66, '200X': 68.27, '400X': 68.44, 'avg_acc': 68.77, 'all_acc': 68.79} | Valid loss: 0.6354878675937653 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.2 mins
2023-12-03_12-56-53: Epoch: 8 | Train loss: 0.6146522335506774 | Train acc: {'40X': 68.76, '100X': 69.77, '200X': 68.19, '400X': 68.47, 'avg_acc': 68.8, 'all_acc': 68.81} | Valid loss: 0.6386051404476166 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.2 mins
2023-12-03_12-58-07: Epoch: 9 | Train loss: 0.6300422764710478 | Train acc: {'40X': 68.76, '100X': 69.77, '200X': 68.36, '400X': 68.47, 'avg_acc': 68.84, 'all_acc': 68.86} | Valid loss: 0.6427739524841308 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.2 mins
2023-12-03_12-59-22: Epoch: 10 | Train loss: 0.6319685733398875 | Train acc: {'40X': 68.73, '100X': 69.77, '200X': 68.29, '400X': 68.38, 'avg_acc': 68.79, 'all_acc': 68.81} | Valid loss: 0.6090906316041946 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.2 mins
2023-12-03_13-00-37: Epoch: 11 | Train loss: 0.5549717918038368 | Train acc: {'40X': 71.05, '100X': 72.55, '200X': 73.03, '400X': 73.3, 'avg_acc': 72.48, 'all_acc': 72.47} | Valid loss: 0.49964314222335815 | Valid acc: {'40X': 79.45, '100X': 76.98, '200X': 82.34, '400X': 77.75, 'avg_acc': 79.13, 'all_acc': 79.14}| Runtime: 1.3 mins
2023-12-03_13-00-37: Train summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  65.97  67.44  65.06  65.35    65.96    65.98
1      2  68.79  69.61  68.27  68.53    68.80    68.81
2      3  67.50  67.98  67.08  66.42    67.24    67.27
3      4  67.89  69.27  67.94  67.43    68.13    68.16
4      5  68.79  69.69  68.27  68.62    68.84    68.86
5      6  68.73  69.66  68.27  68.44    68.77    68.79
6      7  68.76  69.77  68.19  68.47    68.80    68.81
7      8  68.76  69.77  68.36  68.47    68.84    68.86
8      9  68.73  69.77  68.29  68.38    68.79    68.81
9     10  71.05  72.55  73.03  73.30    72.48    72.47
2023-12-03_13-00-37: Eval summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  68.42  67.39  68.16  63.19    66.79    66.88
1      2  68.42  67.39  68.16  63.19    66.79    66.88
2      3  68.42  67.39  68.16  63.19    66.79    66.88
3      4  68.42  67.39  68.16  63.19    66.79    66.88
4      5  68.42  67.39  68.16  63.19    66.79    66.88
5      6  68.42  67.39  68.16  63.19    66.79    66.88
6      7  68.42  67.39  68.16  63.19    66.79    66.88
7      8  68.42  67.39  68.16  63.19    66.79    66.88
8      9  68.42  67.39  68.16  63.19    66.79    66.88
9     10  79.45  76.98  82.34  77.75    79.13    79.14
2023-12-03_13-00-37: Final test accuracy: {'40X': 81.2, '100X': 76.2, '200X': 87.1, '400X': 78.85, 'avg_acc': 80.84, 'all_acc': 80.85}
