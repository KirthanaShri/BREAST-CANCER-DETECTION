2023-12-03_14-02-51: config: {'n_epochs': 10, 'batch_size': 32, 'h': 224, 'w': 224, 'lr': 1e-05, 'optimizer': 'adam', 'momentum': 0.9, 'device': 'cuda', 'n_gpus': 2, 'kernel_size': 3, 'flatten': False, 'model': 'resnet', 'num_blocks_list': [3, 2, 3, 4], 'is_batchnorm': False}
2023-12-03_14-02-51: conv1.0.weight: torch.Size([64, 3, 3, 3])
2023-12-03_14-02-51: conv2_x.0.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_14-02-51: conv2_x.0.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_14-02-51: conv2_x.1.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_14-02-51: conv2_x.1.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_14-02-51: conv2_x.2.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_14-02-51: conv2_x.2.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_14-02-51: conv3_x.0.sequence.0.weight: torch.Size([128, 64, 3, 3])
2023-12-03_14-02-51: conv3_x.0.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_14-02-51: conv3_x.0.shortcut.0.weight: torch.Size([128, 64, 1, 1])
2023-12-03_14-02-51: conv3_x.1.sequence.0.weight: torch.Size([128, 128, 3, 3])
2023-12-03_14-02-51: conv3_x.1.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_14-02-51: conv4_x.0.sequence.0.weight: torch.Size([256, 128, 3, 3])
2023-12-03_14-02-51: conv4_x.0.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_14-02-51: conv4_x.0.shortcut.0.weight: torch.Size([256, 128, 1, 1])
2023-12-03_14-02-51: conv4_x.1.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_14-02-51: conv4_x.1.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_14-02-51: conv4_x.2.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_14-02-51: conv4_x.2.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_14-02-51: conv5_x.0.sequence.0.weight: torch.Size([512, 256, 3, 3])
2023-12-03_14-02-51: conv5_x.0.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_14-02-51: conv5_x.0.shortcut.0.weight: torch.Size([512, 256, 1, 1])
2023-12-03_14-02-51: conv5_x.1.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_14-02-51: conv5_x.1.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_14-02-51: conv5_x.2.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_14-02-51: conv5_x.2.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_14-02-51: conv5_x.3.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_14-02-51: conv5_x.3.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_14-02-51: fc.weight: torch.Size([2, 512])
2023-12-03_14-02-51: fc.bias: torch.Size([2])
2023-12-03_14-02-51: 
Total parameters: 21,850,818;	Trainable: 21,850,818
2023-12-03_14-04-06: Epoch: 2 | Train loss: 93.31696080517125 | Train acc: {'40X': 71.36, '100X': 69.18, '200X': 75.31, '400X': 71.26, 'avg_acc': 71.78, 'all_acc': 71.77} | Valid loss: 27.19448299407959 | Valid acc: {'40X': 41.1, '100X': 41.01, '200X': 44.78, '400X': 48.9, 'avg_acc': 43.95, 'all_acc': 43.81}| Runtime: 1.2 mins
2023-12-03_14-05-22: Epoch: 3 | Train loss: 10.053983594920185 | Train acc: {'40X': 73.37, '100X': 73.31, '200X': 77.81, '400X': 75.5, 'avg_acc': 75.0, 'all_acc': 74.98} | Valid loss: 46.903754539489746 | Valid acc: {'40X': 68.67, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.85, 'all_acc': 66.94}| Runtime: 1.3 mins
2023-12-03_14-06-37: Epoch: 4 | Train loss: 9.877698086403512 | Train acc: {'40X': 76.03, '100X': 75.6, '200X': 78.94, '400X': 79.84, 'avg_acc': 77.6, 'all_acc': 77.53} | Valid loss: 4.407339136600495 | Valid acc: {'40X': 82.21, '100X': 83.69, '200X': 84.83, '400X': 86.26, 'avg_acc': 84.25, 'all_acc': 84.2}| Runtime: 1.3 mins
2023-12-03_14-07-52: Epoch: 5 | Train loss: 4.1874398515257685 | Train acc: {'40X': 77.64, '100X': 75.64, '200X': 81.61, '400X': 82.03, 'avg_acc': 79.23, 'all_acc': 79.14} | Valid loss: 1.824794876202941 | Valid acc: {'40X': 78.45, '100X': 82.97, '200X': 84.33, '400X': 86.26, 'avg_acc': 83.0, 'all_acc': 82.93}| Runtime: 1.2 mins
2023-12-03_14-09-06: Epoch: 6 | Train loss: 2.249325040639756 | Train acc: {'40X': 75.42, '100X': 79.26, '200X': 83.32, '400X': 83.41, 'avg_acc': 80.35, 'all_acc': 80.28} | Valid loss: 1.9860985743999482 | Valid acc: {'40X': 73.68, '100X': 82.01, '200X': 84.33, '400X': 85.16, 'avg_acc': 81.29, 'all_acc': 81.23}| Runtime: 1.2 mins
2023-12-03_14-10-22: Epoch: 7 | Train loss: 3.221105856367865 | Train acc: {'40X': 75.88, '100X': 78.31, '200X': 81.19, '400X': 83.03, 'avg_acc': 79.6, 'all_acc': 79.52} | Valid loss: 2.6995517110824583 | Valid acc: {'40X': 63.91, '100X': 71.94, '200X': 74.63, '400X': 80.77, 'avg_acc': 72.81, 'all_acc': 72.63}| Runtime: 1.3 mins
2023-12-03_14-11-37: Epoch: 8 | Train loss: 1.7604765615652542 | Train acc: {'40X': 81.54, '100X': 80.47, '200X': 84.3, '400X': 85.79, 'avg_acc': 83.02, 'all_acc': 82.94} | Valid loss: 1.1295638097822667 | Valid acc: {'40X': 82.96, '100X': 80.82, '200X': 88.31, '400X': 84.62, 'avg_acc': 84.18, 'all_acc': 84.13}| Runtime: 1.3 mins
2023-12-03_14-12-55: Epoch: 9 | Train loss: 1.6011995886927597 | Train acc: {'40X': 80.32, '100X': 81.06, '200X': 84.66, '400X': 84.77, 'avg_acc': 82.7, 'all_acc': 82.64} | Valid loss: 1.3717301189899445 | Valid acc: {'40X': 82.46, '100X': 83.93, '200X': 87.31, '400X': 87.91, 'avg_acc': 85.4, 'all_acc': 85.34}| Runtime: 1.3 mins
2023-12-03_14-14-11: Epoch: 10 | Train loss: 1.5374837466704383 | Train acc: {'40X': 80.52, '100X': 82.5, '200X': 85.39, '400X': 84.48, 'avg_acc': 83.22, 'all_acc': 83.19} | Valid loss: 1.9763587999343872 | Valid acc: {'40X': 80.45, '100X': 82.97, '200X': 87.56, '400X': 85.44, 'avg_acc': 84.1, 'all_acc': 84.07}| Runtime: 1.3 mins
2023-12-03_14-15-26: Epoch: 11 | Train loss: 1.815608767289165 | Train acc: {'40X': 78.61, '100X': 78.54, '200X': 83.72, '400X': 84.6, 'avg_acc': 81.37, 'all_acc': 81.27} | Valid loss: 2.1275175400078297 | Valid acc: {'40X': 79.45, '100X': 81.53, '200X': 85.07, '400X': 79.95, 'avg_acc': 81.5, 'all_acc': 81.54}| Runtime: 1.3 mins
2023-12-03_14-15-26: Train summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  71.36  69.18  75.31  71.26    71.78    71.77
1      2  73.37  73.31  77.81  75.50    75.00    74.98
2      3  76.03  75.60  78.94  79.84    77.60    77.53
3      4  77.64  75.64  81.61  82.03    79.23    79.14
4      5  75.42  79.26  83.32  83.41    80.35    80.28
5      6  75.88  78.31  81.19  83.03    79.60    79.52
6      7  81.54  80.47  84.30  85.79    83.02    82.94
7      8  80.32  81.06  84.66  84.77    82.70    82.64
8      9  80.52  82.50  85.39  84.48    83.22    83.19
9     10  78.61  78.54  83.72  84.60    81.37    81.27
2023-12-03_14-15-26: Eval summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  41.10  41.01  44.78  48.90    43.95    43.81
1      2  68.67  67.39  68.16  63.19    66.85    66.94
2      3  82.21  83.69  84.83  86.26    84.25    84.20
3      4  78.45  82.97  84.33  86.26    83.00    82.93
4      5  73.68  82.01  84.33  85.16    81.29    81.23
5      6  63.91  71.94  74.63  80.77    72.81    72.63
6      7  82.96  80.82  88.31  84.62    84.18    84.13
7      8  82.46  83.93  87.31  87.91    85.40    85.34
8      9  80.45  82.97  87.56  85.44    84.10    84.07
9     10  79.45  81.53  85.07  79.95    81.50    81.54
2023-12-03_14-15-26: Final test accuracy: {'40X': 85.46, '100X': 81.49, '200X': 90.57, '400X': 87.64, 'avg_acc': 86.29, 'all_acc': 86.22}
