2023-12-03_13-15-22: config: {'n_epochs': 10, 'batch_size': 32, 'h': 224, 'w': 224, 'lr': 0.0001, 'optimizer': 'adam', 'momentum': 0.9, 'device': 'cuda', 'n_gpus': 2, 'kernel_size': 3, 'flatten': False, 'model': 'resnet', 'num_blocks_list': [2, 2, 2, 2], 'is_batchnorm': False}
2023-12-03_13-15-22: conv1.0.weight: torch.Size([64, 3, 3, 3])
2023-12-03_13-15-22: conv2_x.0.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-15-22: conv2_x.0.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-15-22: conv2_x.1.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-15-22: conv2_x.1.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-15-22: conv3_x.0.sequence.0.weight: torch.Size([128, 64, 3, 3])
2023-12-03_13-15-22: conv3_x.0.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-15-22: conv3_x.0.shortcut.0.weight: torch.Size([128, 64, 1, 1])
2023-12-03_13-15-22: conv3_x.1.sequence.0.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-15-22: conv3_x.1.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-15-22: conv4_x.0.sequence.0.weight: torch.Size([256, 128, 3, 3])
2023-12-03_13-15-22: conv4_x.0.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-15-22: conv4_x.0.shortcut.0.weight: torch.Size([256, 128, 1, 1])
2023-12-03_13-15-22: conv4_x.1.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-15-22: conv4_x.1.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-15-22: conv5_x.0.sequence.0.weight: torch.Size([512, 256, 3, 3])
2023-12-03_13-15-22: conv5_x.0.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-15-22: conv5_x.0.shortcut.0.weight: torch.Size([512, 256, 1, 1])
2023-12-03_13-15-22: conv5_x.1.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-15-22: conv5_x.1.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-15-22: fc.weight: torch.Size([2, 512])
2023-12-03_13-15-22: fc.bias: torch.Size([2])
2023-12-03_13-15-22: 
Total parameters: 11,160,258;	Trainable: 11,160,258
2023-12-03_13-16-22: Epoch: 2 | Train loss: 30.757042814046144 | Train acc: {'40X': 74.64, '100X': 75.82, '200X': 79.1, '400X': 78.26, 'avg_acc': 76.96, 'all_acc': 76.92} | Valid loss: 0.5031973019242286 | Valid acc: {'40X': 78.95, '100X': 78.66, '200X': 81.09, '400X': 75.82, 'avg_acc': 78.63, 'all_acc': 78.7}| Runtime: 1.0 mins
2023-12-03_13-17-22: Epoch: 3 | Train loss: 0.4295418012786556 | Train acc: {'40X': 79.21, '100X': 81.8, '200X': 84.0, '400X': 84.13, 'avg_acc': 82.28, 'all_acc': 82.24} | Valid loss: 0.4973691526055336 | Valid acc: {'40X': 81.45, '100X': 81.06, '200X': 84.58, '400X': 82.97, 'avg_acc': 82.51, 'all_acc': 82.49}| Runtime: 1.0 mins
2023-12-03_13-18-22: Epoch: 4 | Train loss: 0.39193657765517365 | Train acc: {'40X': 79.55, '100X': 81.4, '200X': 86.18, '400X': 84.93, 'avg_acc': 83.02, 'all_acc': 82.96} | Valid loss: 0.35459419280290605 | Valid acc: {'40X': 83.96, '100X': 82.97, '200X': 86.57, '400X': 83.79, 'avg_acc': 84.32, 'all_acc': 84.32}| Runtime: 1.0 mins
2023-12-03_13-19-22: Epoch: 5 | Train loss: 0.38766254278252255 | Train acc: {'40X': 79.5, '100X': 81.61, '200X': 85.49, '400X': 85.78, 'avg_acc': 83.1, 'all_acc': 83.02} | Valid loss: 0.5214059859514236 | Valid acc: {'40X': 80.2, '100X': 80.58, '200X': 84.58, '400X': 82.69, 'avg_acc': 82.01, 'all_acc': 81.98}| Runtime: 1.0 mins
2023-12-03_13-20-22: Epoch: 6 | Train loss: 0.32704443074259404 | Train acc: {'40X': 83.74, '100X': 83.39, '200X': 87.57, '400X': 88.35, 'avg_acc': 85.76, 'all_acc': 85.68} | Valid loss: 0.3457259230315685 | Valid acc: {'40X': 84.96, '100X': 82.25, '200X': 87.56, '400X': 85.71, 'avg_acc': 85.12, 'all_acc': 85.08}| Runtime: 1.0 mins
2023-12-03_13-21-23: Epoch: 7 | Train loss: 0.3394415807482359 | Train acc: {'40X': 84.35, '100X': 83.69, '200X': 86.22, '400X': 88.08, 'avg_acc': 85.58, 'all_acc': 85.52} | Valid loss: 0.338594913482666 | Valid acc: {'40X': 84.46, '100X': 83.93, '200X': 86.07, '400X': 87.09, 'avg_acc': 85.39, 'all_acc': 85.34}| Runtime: 1.0 mins
2023-12-03_13-22-24: Epoch: 8 | Train loss: 0.33089967253240377 | Train acc: {'40X': 83.42, '100X': 85.07, '200X': 87.56, '400X': 88.07, 'avg_acc': 86.03, 'all_acc': 85.98} | Valid loss: 0.32242128923535346 | Valid acc: {'40X': 85.71, '100X': 85.37, '200X': 88.56, '400X': 88.46, 'avg_acc': 87.02, 'all_acc': 86.98}| Runtime: 1.0 mins
2023-12-03_13-23-23: Epoch: 9 | Train loss: 0.29646764197260944 | Train acc: {'40X': 84.94, '100X': 85.86, '200X': 89.8, '400X': 89.36, 'avg_acc': 87.49, 'all_acc': 87.44} | Valid loss: 0.4500681805610657 | Valid acc: {'40X': 71.18, '100X': 78.42, '200X': 82.59, '400X': 80.22, 'avg_acc': 78.1, 'all_acc': 78.07}| Runtime: 1.0 mins
2023-12-03_13-24-23: Epoch: 10 | Train loss: 0.27130727792108383 | Train acc: {'40X': 85.52, '100X': 87.22, '200X': 90.06, '400X': 88.81, 'avg_acc': 87.9, 'all_acc': 87.88} | Valid loss: 0.44196212202310564 | Valid acc: {'40X': 81.45, '100X': 81.53, '200X': 84.83, '400X': 83.24, 'avg_acc': 82.76, 'all_acc': 82.74}| Runtime: 1.0 mins
2023-12-03_13-25-23: Epoch: 11 | Train loss: 0.27865059391872304 | Train acc: {'40X': 84.1, '100X': 86.75, '200X': 90.3, '400X': 90.55, 'avg_acc': 87.92, 'all_acc': 87.86} | Valid loss: 0.3039985936880112 | Valid acc: {'40X': 85.46, '100X': 84.41, '200X': 88.81, '400X': 88.74, 'avg_acc': 86.86, 'all_acc': 86.79}| Runtime: 1.0 mins
2023-12-03_13-25-23: Train summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  74.64  75.82  79.10  78.26    76.96    76.92
1      2  79.21  81.80  84.00  84.13    82.28    82.24
2      3  79.55  81.40  86.18  84.93    83.02    82.96
3      4  79.50  81.61  85.49  85.78    83.10    83.02
4      5  83.74  83.39  87.57  88.35    85.76    85.68
5      6  84.35  83.69  86.22  88.08    85.58    85.52
6      7  83.42  85.07  87.56  88.07    86.03    85.98
7      8  84.94  85.86  89.80  89.36    87.49    87.44
8      9  85.52  87.22  90.06  88.81    87.90    87.88
9     10  84.10  86.75  90.30  90.55    87.92    87.86
2023-12-03_13-25-23: Eval summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  78.95  78.66  81.09  75.82    78.63    78.70
1      2  81.45  81.06  84.58  82.97    82.51    82.49
2      3  83.96  82.97  86.57  83.79    84.32    84.32
3      4  80.20  80.58  84.58  82.69    82.01    81.98
4      5  84.96  82.25  87.56  85.71    85.12    85.08
5      6  84.46  83.93  86.07  87.09    85.39    85.34
6      7  85.71  85.37  88.56  88.46    87.02    86.98
7      8  71.18  78.42  82.59  80.22    78.10    78.07
8      9  81.45  81.53  84.83  83.24    82.76    82.74
9     10  85.46  84.41  88.81  88.74    86.86    86.79
2023-12-03_13-25-23: Final test accuracy: {'40X': 86.72, '100X': 82.69, '200X': 92.56, '400X': 89.01, 'avg_acc': 87.74, 'all_acc': 87.67}
