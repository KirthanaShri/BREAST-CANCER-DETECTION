2023-12-03_13-25-24: config: {'n_epochs': 10, 'batch_size': 32, 'h': 224, 'w': 224, 'lr': 0.0001, 'optimizer': 'adam', 'momentum': 0.9, 'device': 'cuda', 'n_gpus': 2, 'kernel_size': 3, 'flatten': False, 'model': 'resnet', 'num_blocks_list': [3, 2, 3, 4], 'is_batchnorm': False}
2023-12-03_13-25-24: conv1.0.weight: torch.Size([64, 3, 3, 3])
2023-12-03_13-25-24: conv2_x.0.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-25-24: conv2_x.0.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-25-24: conv2_x.1.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-25-24: conv2_x.1.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-25-24: conv2_x.2.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-25-24: conv2_x.2.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-25-24: conv3_x.0.sequence.0.weight: torch.Size([128, 64, 3, 3])
2023-12-03_13-25-24: conv3_x.0.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-25-24: conv3_x.0.shortcut.0.weight: torch.Size([128, 64, 1, 1])
2023-12-03_13-25-24: conv3_x.1.sequence.0.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-25-24: conv3_x.1.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-25-24: conv4_x.0.sequence.0.weight: torch.Size([256, 128, 3, 3])
2023-12-03_13-25-24: conv4_x.0.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-25-24: conv4_x.0.shortcut.0.weight: torch.Size([256, 128, 1, 1])
2023-12-03_13-25-24: conv4_x.1.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-25-24: conv4_x.1.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-25-24: conv4_x.2.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-25-24: conv4_x.2.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-25-24: conv5_x.0.sequence.0.weight: torch.Size([512, 256, 3, 3])
2023-12-03_13-25-24: conv5_x.0.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-25-24: conv5_x.0.shortcut.0.weight: torch.Size([512, 256, 1, 1])
2023-12-03_13-25-24: conv5_x.1.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-25-24: conv5_x.1.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-25-24: conv5_x.2.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-25-24: conv5_x.2.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-25-24: conv5_x.3.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-25-24: conv5_x.3.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-25-24: fc.weight: torch.Size([2, 512])
2023-12-03_13-25-24: fc.bias: torch.Size([2])
2023-12-03_13-25-24: 
Total parameters: 21,850,818;	Trainable: 21,850,818
2023-12-03_13-26-39: Epoch: 2 | Train loss: 82.49006007490931 | Train acc: {'40X': 71.66, '100X': 72.43, '200X': 78.46, '400X': 76.58, 'avg_acc': 74.78, 'all_acc': 74.73} | Valid loss: 0.49512777626514437 | Valid acc: {'40X': 68.42, '100X': 71.7, '200X': 77.61, '400X': 80.77, 'avg_acc': 74.62, 'all_acc': 74.46}| Runtime: 1.3 mins
2023-12-03_13-27-54: Epoch: 3 | Train loss: 0.48361264461198367 | Train acc: {'40X': 76.65, '100X': 79.37, '200X': 82.09, '400X': 81.54, 'avg_acc': 79.91, 'all_acc': 79.88} | Valid loss: 0.38527790158987046 | Valid acc: {'40X': 82.96, '100X': 81.53, '200X': 86.07, '400X': 84.07, 'avg_acc': 83.66, 'all_acc': 83.63}| Runtime: 1.3 mins
2023-12-03_13-29-10: Epoch: 4 | Train loss: 0.41324925991530354 | Train acc: {'40X': 78.86, '100X': 82.41, '200X': 84.55, '400X': 85.5, 'avg_acc': 82.83, 'all_acc': 82.77} | Valid loss: 0.3822190237045288 | Valid acc: {'40X': 82.21, '100X': 82.25, '200X': 86.07, '400X': 84.07, 'avg_acc': 83.65, 'all_acc': 83.63}| Runtime: 1.3 mins
2023-12-03_13-30-26: Epoch: 5 | Train loss: 0.3568658247791432 | Train acc: {'40X': 81.61, '100X': 82.65, '200X': 87.4, '400X': 86.59, 'avg_acc': 84.56, 'all_acc': 84.5} | Valid loss: 0.38866124525666235 | Valid acc: {'40X': 82.21, '100X': 82.01, '200X': 86.07, '400X': 85.44, 'avg_acc': 83.93, 'all_acc': 83.88}| Runtime: 1.3 mins
2023-12-03_13-31-41: Epoch: 6 | Train loss: 0.35153861635842837 | Train acc: {'40X': 82.01, '100X': 82.5, '200X': 85.96, '400X': 86.62, 'avg_acc': 84.27, 'all_acc': 84.21} | Valid loss: 0.35855815798044205 | Valid acc: {'40X': 84.21, '100X': 83.69, '200X': 86.57, '400X': 85.44, 'avg_acc': 84.98, 'all_acc': 84.96}| Runtime: 1.3 mins
2023-12-03_13-32-56: Epoch: 7 | Train loss: 0.3278846483979676 | Train acc: {'40X': 82.68, '100X': 85.07, '200X': 87.31, '400X': 88.43, 'avg_acc': 85.87, 'all_acc': 85.81} | Valid loss: 0.3270456051826477 | Valid acc: {'40X': 85.21, '100X': 85.61, '200X': 88.81, '400X': 85.44, 'avg_acc': 86.27, 'all_acc': 86.28}| Runtime: 1.3 mins
2023-12-03_13-34-12: Epoch: 8 | Train loss: 0.31790769437479005 | Train acc: {'40X': 84.74, '100X': 84.84, '200X': 88.46, '400X': 87.44, 'avg_acc': 86.37, 'all_acc': 86.34} | Valid loss: 0.3119526319205761 | Valid acc: {'40X': 84.71, '100X': 83.93, '200X': 89.8, '400X': 86.54, 'avg_acc': 86.24, 'all_acc': 86.22}| Runtime: 1.3 mins
2023-12-03_13-35-27: Epoch: 9 | Train loss: 0.2963541229129643 | Train acc: {'40X': 84.17, '100X': 86.76, '200X': 88.63, '400X': 89.46, 'avg_acc': 87.26, 'all_acc': 87.2} | Valid loss: 0.36277401268482207 | Valid acc: {'40X': 82.21, '100X': 82.73, '200X': 87.81, '400X': 84.34, 'avg_acc': 84.27, 'all_acc': 84.26}| Runtime: 1.2 mins
2023-12-03_13-36-42: Epoch: 10 | Train loss: 0.30433067552292264 | Train acc: {'40X': 83.67, '100X': 86.68, '200X': 88.56, '400X': 88.72, 'avg_acc': 86.91, 'all_acc': 86.87} | Valid loss: 0.41795739352703093 | Valid acc: {'40X': 80.95, '100X': 81.77, '200X': 85.57, '400X': 84.07, 'avg_acc': 83.09, 'all_acc': 83.06}| Runtime: 1.3 mins
2023-12-03_13-37-56: Epoch: 11 | Train loss: 0.2702862600216994 | Train acc: {'40X': 86.04, '100X': 86.32, '200X': 89.97, '400X': 90.1, 'avg_acc': 88.11, 'all_acc': 88.05} | Valid loss: 0.3282215803861618 | Valid acc: {'40X': 81.95, '100X': 84.89, '200X': 87.81, '400X': 88.19, 'avg_acc': 85.71, 'all_acc': 85.65}| Runtime: 1.2 mins
2023-12-03_13-37-56: Train summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  71.66  72.43  78.46  76.58    74.78    74.73
1      2  76.65  79.37  82.09  81.54    79.91    79.88
2      3  78.86  82.41  84.55  85.50    82.83    82.77
3      4  81.61  82.65  87.40  86.59    84.56    84.50
4      5  82.01  82.50  85.96  86.62    84.27    84.21
5      6  82.68  85.07  87.31  88.43    85.87    85.81
6      7  84.74  84.84  88.46  87.44    86.37    86.34
7      8  84.17  86.76  88.63  89.46    87.26    87.20
8      9  83.67  86.68  88.56  88.72    86.91    86.87
9     10  86.04  86.32  89.97  90.10    88.11    88.05
2023-12-03_13-37-56: Eval summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  68.42  71.70  77.61  80.77    74.62    74.46
1      2  82.96  81.53  86.07  84.07    83.66    83.63
2      3  82.21  82.25  86.07  84.07    83.65    83.63
3      4  82.21  82.01  86.07  85.44    83.93    83.88
4      5  84.21  83.69  86.57  85.44    84.98    84.96
5      6  85.21  85.61  88.81  85.44    86.27    86.28
6      7  84.71  83.93  89.80  86.54    86.24    86.22
7      8  82.21  82.73  87.81  84.34    84.27    84.26
8      9  80.95  81.77  85.57  84.07    83.09    83.06
9     10  81.95  84.89  87.81  88.19    85.71    85.65
2023-12-03_13-37-56: Final test accuracy: {'40X': 83.96, '100X': 84.13, '200X': 90.82, '400X': 87.64, 'avg_acc': 86.64, 'all_acc': 86.6}
