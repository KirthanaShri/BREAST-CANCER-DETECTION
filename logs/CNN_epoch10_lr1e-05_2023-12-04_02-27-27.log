2023-12-04_02-27-27: config: {'n_epochs': 10, 'batch_size': 32, 'h': 224, 'w': 224, 'lr': 1e-05, 'optimizer': 'sgd', 'momentum': 0.9, 'weight_decay': 0, 'device': 'cuda:1', 'n_gpus': 1, 'kernel_size': 3, 'model': 'CNN', 'use_pooling': True, 'num_kernel_conv_list': [32, 64, 64, 64]}
2023-12-04_02-27-27: conv1.weight: torch.Size([32, 3, 3, 3])
2023-12-04_02-27-27: conv1.bias: torch.Size([32])
2023-12-04_02-27-27: conv_layers.0.weight: torch.Size([64, 32, 3, 3])
2023-12-04_02-27-27: conv_layers.0.bias: torch.Size([64])
2023-12-04_02-27-27: conv_layers.3.weight: torch.Size([64, 64, 3, 3])
2023-12-04_02-27-27: conv_layers.3.bias: torch.Size([64])
2023-12-04_02-27-27: conv_layers.6.weight: torch.Size([64, 64, 3, 3])
2023-12-04_02-27-27: conv_layers.6.bias: torch.Size([64])
2023-12-04_02-27-27: linear.weight: torch.Size([2, 50176])
2023-12-04_02-27-27: linear.bias: torch.Size([2])
2023-12-04_02-27-27: 
Total parameters: 193,602;	Trainable: 193,602
2023-12-04_02-28-30: Epoch: 1 | Train loss: 0.6477508118023744 | Train acc: {'40X': 68.76, '100X': 69.58, '200X': 68.41, '400X': 68.17, 'avg_acc': 68.73, 'all_acc': 68.75} | Valid loss: 0.6361783170700073 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.0 mins
2023-12-04_02-29-34: Epoch: 2 | Train loss: 0.6191883777847161 | Train acc: {'40X': 68.76, '100X': 69.72, '200X': 68.38, '400X': 68.5, 'avg_acc': 68.84, 'all_acc': 68.86} | Valid loss: 0.6297730189561844 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.1 mins
2023-12-04_02-30-36: Epoch: 3 | Train loss: 0.6131254078165905 | Train acc: {'40X': 68.81, '100X': 69.66, '200X': 68.24, '400X': 68.47, 'avg_acc': 68.79, 'all_acc': 68.81} | Valid loss: 0.6279068875312805 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.0 mins
2023-12-04_02-31-37: Epoch: 4 | Train loss: 0.6069165063468186 | Train acc: {'40X': 68.79, '100X': 69.77, '200X': 68.33, '400X': 68.57, 'avg_acc': 68.86, 'all_acc': 68.88} | Valid loss: 0.6199557554721832 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.0 mins
2023-12-04_02-32-39: Epoch: 5 | Train loss: 0.6010572354535799 | Train acc: {'40X': 68.79, '100X': 69.75, '200X': 68.35, '400X': 68.47, 'avg_acc': 68.84, 'all_acc': 68.86} | Valid loss: 0.6123252338171006 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.0 mins
2023-12-04_02-33-43: Epoch: 6 | Train loss: 0.5947639684419375 | Train acc: {'40X': 68.87, '100X': 69.72, '200X': 68.27, '400X': 68.41, 'avg_acc': 68.82, 'all_acc': 68.83} | Valid loss: 0.6063775354623795 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.1 mins
2023-12-04_02-34-45: Epoch: 7 | Train loss: 0.5879207960254437 | Train acc: {'40X': 68.73, '100X': 69.69, '200X': 68.33, '400X': 68.44, 'avg_acc': 68.8, 'all_acc': 68.81} | Valid loss: 0.5982598179578781 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.0 mins
2023-12-04_02-35-48: Epoch: 8 | Train loss: 0.5800997888719713 | Train acc: {'40X': 68.68, '100X': 69.77, '200X': 68.36, '400X': 68.47, 'avg_acc': 68.82, 'all_acc': 68.83} | Valid loss: 0.5903518635034561 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.0 mins
2023-12-04_02-36-51: Epoch: 9 | Train loss: 0.5713444901881991 | Train acc: {'40X': 68.87, '100X': 69.64, '200X': 68.38, '400X': 68.47, 'avg_acc': 68.84, 'all_acc': 68.86} | Valid loss: 0.5802419435977936 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.1 mins
2023-12-04_02-37-53: Epoch: 10 | Train loss: 0.5626148132456316 | Train acc: {'40X': 68.76, '100X': 69.61, '200X': 68.19, '400X': 68.56, 'avg_acc': 68.78, 'all_acc': 68.79} | Valid loss: 0.5705420392751693 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.0 mins
2023-12-04_02-37-53: Train summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  68.76  69.58  68.41  68.17    68.73    68.75
1      2  68.76  69.72  68.38  68.50    68.84    68.86
2      3  68.81  69.66  68.24  68.47    68.79    68.81
3      4  68.79  69.77  68.33  68.57    68.86    68.88
4      5  68.79  69.75  68.35  68.47    68.84    68.86
5      6  68.87  69.72  68.27  68.41    68.82    68.83
6      7  68.73  69.69  68.33  68.44    68.80    68.81
7      8  68.68  69.77  68.36  68.47    68.82    68.83
8      9  68.87  69.64  68.38  68.47    68.84    68.86
9     10  68.76  69.61  68.19  68.56    68.78    68.79
2023-12-04_02-37-53: Eval summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  68.42  67.39  68.16  63.19    66.79    66.88
1      2  68.42  67.39  68.16  63.19    66.79    66.88
2      3  68.42  67.39  68.16  63.19    66.79    66.88
3      4  68.42  67.39  68.16  63.19    66.79    66.88
4      5  68.42  67.39  68.16  63.19    66.79    66.88
5      6  68.42  67.39  68.16  63.19    66.79    66.88
6      7  68.42  67.39  68.16  63.19    66.79    66.88
7      8  68.42  67.39  68.16  63.19    66.79    66.88
8      9  68.42  67.39  68.16  63.19    66.79    66.88
9     10  68.42  67.39  68.16  63.19    66.79    66.88
2023-12-04_02-37-53: Final test accuracy: {'40X': 68.67, '100X': 68.75, '200X': 72.21, '400X': 69.78, 'avg_acc': 69.85, 'all_acc': 69.85}
