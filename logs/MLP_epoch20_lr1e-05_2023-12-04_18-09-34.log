2023-12-04_18-09-34: config: {'n_epochs': 20, 'batch_size': 32, 'h': 224, 'w': 224, 'lr': 1e-05, 'optimizer': 'adam', 'momentum': 0.9, 'weight_decay': 0, 'device': 'cuda:0', 'n_gpus': 1, 'model': 'MLP', 'h_dim_list': [80, 50, 40]}
2023-12-04_18-09-34: layers.0.weight: torch.Size([80, 150528])
2023-12-04_18-09-34: layers.0.bias: torch.Size([80])
2023-12-04_18-09-34: layers.2.weight: torch.Size([50, 80])
2023-12-04_18-09-34: layers.2.bias: torch.Size([50])
2023-12-04_18-09-34: layers.4.weight: torch.Size([40, 50])
2023-12-04_18-09-34: layers.4.bias: torch.Size([40])
2023-12-04_18-09-34: layers.6.weight: torch.Size([2, 40])
2023-12-04_18-09-34: layers.6.bias: torch.Size([2])
2023-12-04_18-09-34: 
Total parameters: 12,048,492;	Trainable: 12,048,492
