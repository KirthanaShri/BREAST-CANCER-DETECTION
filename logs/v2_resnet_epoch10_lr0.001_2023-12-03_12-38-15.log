2023-12-03_12-38-15: config: {'n_epochs': 10, 'batch_size': 32, 'h': 224, 'w': 224, 'lr': 0.001, 'optimizer': 'adam', 'momentum': 0.9, 'device': 'cuda', 'n_gpus': 2, 'kernel_size': 3, 'flatten': False, 'model': 'resnet', 'num_blocks_list': [2, 2, 2, 2], 'is_batchnorm': False}
2023-12-03_12-38-15: conv1.0.weight: torch.Size([64, 3, 3, 3])
2023-12-03_12-38-15: conv2_x.0.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_12-38-15: conv2_x.0.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_12-38-15: conv2_x.1.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_12-38-15: conv2_x.1.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_12-38-15: conv3_x.0.sequence.0.weight: torch.Size([128, 64, 3, 3])
2023-12-03_12-38-15: conv3_x.0.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_12-38-15: conv3_x.0.shortcut.0.weight: torch.Size([128, 64, 1, 1])
2023-12-03_12-38-15: conv3_x.1.sequence.0.weight: torch.Size([128, 128, 3, 3])
2023-12-03_12-38-15: conv3_x.1.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_12-38-15: conv4_x.0.sequence.0.weight: torch.Size([256, 128, 3, 3])
2023-12-03_12-38-15: conv4_x.0.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_12-38-15: conv4_x.0.shortcut.0.weight: torch.Size([256, 128, 1, 1])
2023-12-03_12-38-15: conv4_x.1.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_12-38-15: conv4_x.1.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_12-38-15: conv5_x.0.sequence.0.weight: torch.Size([512, 256, 3, 3])
2023-12-03_12-38-15: conv5_x.0.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_12-38-15: conv5_x.0.shortcut.0.weight: torch.Size([512, 256, 1, 1])
2023-12-03_12-38-15: conv5_x.1.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_12-38-15: conv5_x.1.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_12-38-15: fc.weight: torch.Size([2, 512])
2023-12-03_12-38-15: fc.bias: torch.Size([2])
2023-12-03_12-38-15: 
Total parameters: 11,160,258;	Trainable: 11,160,258
2023-12-03_12-39-18: Epoch: 2 | Train loss: 1994.779444531613 | Train acc: {'40X': 66.78, '100X': 68.67, '200X': 65.09, '400X': 66.24, 'avg_acc': 66.69, 'all_acc': 66.72} | Valid loss: 0.5774549180269242 | Valid acc: {'40X': 68.42, '100X': 67.39, '200X': 68.16, '400X': 63.19, 'avg_acc': 66.79, 'all_acc': 66.88}| Runtime: 1.0 mins
2023-12-03_12-40-17: Epoch: 3 | Train loss: 0.48583950404379816 | Train acc: {'40X': 76.32, '100X': 78.28, '200X': 81.67, '400X': 78.66, 'avg_acc': 78.73, 'all_acc': 78.74} | Valid loss: 0.46288216501474383 | Valid acc: {'40X': 79.2, '100X': 78.42, '200X': 84.58, '400X': 84.62, 'avg_acc': 81.7, 'all_acc': 81.61}| Runtime: 1.0 mins
2023-12-03_12-41-16: Epoch: 4 | Train loss: 0.4514802501008317 | Train acc: {'40X': 78.33, '100X': 80.82, '200X': 83.32, '400X': 81.38, 'avg_acc': 80.96, 'all_acc': 80.95} | Valid loss: 0.45345106065273283 | Valid acc: {'40X': 81.2, '100X': 80.34, '200X': 82.84, '400X': 81.87, 'avg_acc': 81.56, 'all_acc': 81.54}| Runtime: 1.0 mins
2023-12-03_12-42-16: Epoch: 5 | Train loss: 0.4338682154948647 | Train acc: {'40X': 79.9, '100X': 79.21, '200X': 84.33, '400X': 82.84, 'avg_acc': 81.57, 'all_acc': 81.52} | Valid loss: 0.43495607376098633 | Valid acc: {'40X': 82.21, '100X': 80.34, '200X': 84.83, '400X': 83.79, 'avg_acc': 82.79, 'all_acc': 82.74}| Runtime: 1.0 mins
2023-12-03_12-43-16: Epoch: 6 | Train loss: 0.43432188779115677 | Train acc: {'40X': 79.48, '100X': 80.43, '200X': 83.72, '400X': 83.41, 'avg_acc': 81.76, 'all_acc': 81.71} | Valid loss: 0.49059401512145995 | Valid acc: {'40X': 73.68, '100X': 80.1, '200X': 80.6, '400X': 76.65, 'avg_acc': 77.76, 'all_acc': 77.81}| Runtime: 1.0 mins
2023-12-03_12-44-16: Epoch: 7 | Train loss: 0.4206853145481767 | Train acc: {'40X': 80.02, '100X': 80.63, '200X': 84.66, '400X': 84.4, 'avg_acc': 82.43, 'all_acc': 82.37} | Valid loss: 0.41046629011631014 | Valid acc: {'40X': 83.46, '100X': 83.93, '200X': 84.58, '400X': 84.07, 'avg_acc': 84.01, 'all_acc': 84.01}| Runtime: 1.0 mins
2023-12-03_12-45-15: Epoch: 8 | Train loss: 0.4005511939726971 | Train acc: {'40X': 81.34, '100X': 81.67, '200X': 85.99, '400X': 84.88, 'avg_acc': 83.47, 'all_acc': 83.42} | Valid loss: 0.38884314507246015 | Valid acc: {'40X': 82.46, '100X': 84.17, '200X': 85.07, '400X': 83.52, 'avg_acc': 83.8, 'all_acc': 83.82}| Runtime: 1.0 mins
2023-12-03_12-46-15: Epoch: 9 | Train loss: 0.3942996627978376 | Train acc: {'40X': 80.96, '100X': 81.93, '200X': 87.08, '400X': 85.53, 'avg_acc': 83.88, 'all_acc': 83.83} | Valid loss: 0.3964844697713852 | Valid acc: {'40X': 80.7, '100X': 81.77, '200X': 85.32, '400X': 83.24, 'avg_acc': 82.76, 'all_acc': 82.74}| Runtime: 1.0 mins
2023-12-03_12-47-15: Epoch: 10 | Train loss: 0.3803242225501988 | Train acc: {'40X': 81.56, '100X': 83.05, '200X': 85.66, '400X': 85.9, 'avg_acc': 84.04, 'all_acc': 83.99} | Valid loss: 0.3839433406293392 | Valid acc: {'40X': 82.46, '100X': 86.09, '200X': 86.32, '400X': 85.16, 'avg_acc': 85.01, 'all_acc': 85.02}| Runtime: 1.0 mins
2023-12-03_12-48-14: Epoch: 11 | Train loss: 0.3742356264107936 | Train acc: {'40X': 82.11, '100X': 84.74, '200X': 86.9, '400X': 86.59, 'avg_acc': 85.08, 'all_acc': 85.05} | Valid loss: 0.4014960864186287 | Valid acc: {'40X': 82.21, '100X': 81.53, '200X': 85.57, '400X': 85.16, 'avg_acc': 83.62, 'all_acc': 83.57}| Runtime: 1.0 mins
2023-12-03_12-48-14: Train summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  66.78  68.67  65.09  66.24    66.69    66.72
1      2  76.32  78.28  81.67  78.66    78.73    78.74
2      3  78.33  80.82  83.32  81.38    80.96    80.95
3      4  79.90  79.21  84.33  82.84    81.57    81.52
4      5  79.48  80.43  83.72  83.41    81.76    81.71
5      6  80.02  80.63  84.66  84.40    82.43    82.37
6      7  81.34  81.67  85.99  84.88    83.47    83.42
7      8  80.96  81.93  87.08  85.53    83.88    83.83
8      9  81.56  83.05  85.66  85.90    84.04    83.99
9     10  82.11  84.74  86.90  86.59    85.08    85.05
2023-12-03_12-48-14: Eval summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  68.42  67.39  68.16  63.19    66.79    66.88
1      2  79.20  78.42  84.58  84.62    81.70    81.61
2      3  81.20  80.34  82.84  81.87    81.56    81.54
3      4  82.21  80.34  84.83  83.79    82.79    82.74
4      5  73.68  80.10  80.60  76.65    77.76    77.81
5      6  83.46  83.93  84.58  84.07    84.01    84.01
6      7  82.46  84.17  85.07  83.52    83.80    83.82
7      8  80.70  81.77  85.32  83.24    82.76    82.74
8      9  82.46  86.09  86.32  85.16    85.01    85.02
9     10  82.21  81.53  85.57  85.16    83.62    83.57
2023-12-03_12-48-14: Final test accuracy: {'40X': 84.46, '100X': 83.41, '200X': 89.58, '400X': 83.52, 'avg_acc': 85.24, 'all_acc': 85.27}
