2023-12-03_13-52-46: config: {'n_epochs': 10, 'batch_size': 32, 'h': 224, 'w': 224, 'lr': 1e-05, 'optimizer': 'adam', 'momentum': 0.9, 'device': 'cuda', 'n_gpus': 2, 'kernel_size': 3, 'flatten': False, 'model': 'resnet', 'num_blocks_list': [2, 2, 2, 2], 'is_batchnorm': False}
2023-12-03_13-52-46: conv1.0.weight: torch.Size([64, 3, 3, 3])
2023-12-03_13-52-46: conv2_x.0.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-52-46: conv2_x.0.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-52-46: conv2_x.1.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-52-46: conv2_x.1.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-52-46: conv3_x.0.sequence.0.weight: torch.Size([128, 64, 3, 3])
2023-12-03_13-52-46: conv3_x.0.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-52-46: conv3_x.0.shortcut.0.weight: torch.Size([128, 64, 1, 1])
2023-12-03_13-52-46: conv3_x.1.sequence.0.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-52-46: conv3_x.1.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-52-46: conv4_x.0.sequence.0.weight: torch.Size([256, 128, 3, 3])
2023-12-03_13-52-46: conv4_x.0.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-52-46: conv4_x.0.shortcut.0.weight: torch.Size([256, 128, 1, 1])
2023-12-03_13-52-46: conv4_x.1.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-52-46: conv4_x.1.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-52-46: conv5_x.0.sequence.0.weight: torch.Size([512, 256, 3, 3])
2023-12-03_13-52-46: conv5_x.0.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-52-46: conv5_x.0.shortcut.0.weight: torch.Size([512, 256, 1, 1])
2023-12-03_13-52-46: conv5_x.1.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-52-46: conv5_x.1.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-52-46: fc.weight: torch.Size([2, 512])
2023-12-03_13-52-46: fc.bias: torch.Size([2])
2023-12-03_13-52-46: 
Total parameters: 11,160,258;	Trainable: 11,160,258
2023-12-03_13-53-46: Epoch: 2 | Train loss: 10.597555047958284 | Train acc: {'40X': 68.2, '100X': 70.33, '200X': 73.43, '400X': 72.45, 'avg_acc': 71.1, 'all_acc': 71.07} | Valid loss: 1.0801671698689461 | Valid acc: {'40X': 83.96, '100X': 79.86, '200X': 83.58, '400X': 82.42, 'avg_acc': 82.46, 'all_acc': 82.43}| Runtime: 1.0 mins
2023-12-03_13-54-46: Epoch: 3 | Train loss: 2.4515899360072533 | Train acc: {'40X': 72.4, '100X': 73.6, '200X': 81.21, '400X': 77.34, 'avg_acc': 76.14, 'all_acc': 76.1} | Valid loss: 3.431139807701111 | Valid acc: {'40X': 78.45, '100X': 79.86, '200X': 81.59, '400X': 77.2, 'avg_acc': 79.28, 'all_acc': 79.33}| Runtime: 1.0 mins
2023-12-03_13-55-47: Epoch: 4 | Train loss: 1.9467167203893532 | Train acc: {'40X': 73.85, '100X': 76.14, '200X': 80.13, '400X': 79.08, 'avg_acc': 77.3, 'all_acc': 77.26} | Valid loss: 0.8560885432362556 | Valid acc: {'40X': 81.2, '100X': 81.77, '200X': 83.08, '400X': 81.87, 'avg_acc': 81.98, 'all_acc': 81.98}| Runtime: 1.0 mins
2023-12-03_13-56-47: Epoch: 5 | Train loss: 1.206607077723822 | Train acc: {'40X': 77.28, '100X': 76.1, '200X': 82.52, '400X': 83.1, 'avg_acc': 79.75, 'all_acc': 79.65} | Valid loss: 0.9355322235822677 | Valid acc: {'40X': 67.17, '100X': 73.86, '200X': 76.87, '400X': 79.95, 'avg_acc': 74.46, 'all_acc': 74.34}| Runtime: 1.0 mins
2023-12-03_13-57-47: Epoch: 6 | Train loss: 1.3694216263153263 | Train acc: {'40X': 77.94, '100X': 76.71, '200X': 80.94, '400X': 80.95, 'avg_acc': 79.13, 'all_acc': 79.08} | Valid loss: 0.5420359860360623 | Valid acc: {'40X': 83.21, '100X': 80.82, '200X': 81.84, '400X': 83.52, 'avg_acc': 82.35, 'all_acc': 82.3}| Runtime: 1.0 mins
2023-12-03_13-58-49: Epoch: 7 | Train loss: 0.8424250430779884 | Train acc: {'40X': 79.11, '100X': 78.17, '200X': 83.15, '400X': 82.17, 'avg_acc': 80.65, 'all_acc': 80.6} | Valid loss: 1.1235258954763412 | Valid acc: {'40X': 81.2, '100X': 81.29, '200X': 84.58, '400X': 84.07, 'avg_acc': 82.78, 'all_acc': 82.74}| Runtime: 1.0 mins
2023-12-03_13-59-50: Epoch: 8 | Train loss: 0.7097824401650075 | Train acc: {'40X': 81.29, '100X': 80.24, '200X': 81.62, '400X': 84.88, 'avg_acc': 82.01, 'all_acc': 81.93} | Valid loss: 0.4563376721739769 | Valid acc: {'40X': 83.96, '100X': 84.65, '200X': 86.32, '400X': 85.99, 'avg_acc': 85.23, 'all_acc': 85.21}| Runtime: 1.0 mins
2023-12-03_14-00-50: Epoch: 9 | Train loss: 0.8934285162752997 | Train acc: {'40X': 76.99, '100X': 78.97, '200X': 83.82, '400X': 84.04, 'avg_acc': 80.96, 'all_acc': 80.87} | Valid loss: 1.332267090678215 | Valid acc: {'40X': 78.7, '100X': 81.06, '200X': 82.09, '400X': 78.85, 'avg_acc': 80.18, 'all_acc': 80.21}| Runtime: 1.0 mins
2023-12-03_14-01-51: Epoch: 10 | Train loss: 0.8632559858111514 | Train acc: {'40X': 77.62, '100X': 80.69, '200X': 83.49, '400X': 83.03, 'avg_acc': 81.21, 'all_acc': 81.17} | Valid loss: 1.7332472932338714 | Valid acc: {'40X': 77.19, '100X': 75.3, '200X': 80.85, '400X': 74.73, 'avg_acc': 77.02, 'all_acc': 77.05}| Runtime: 1.0 mins
2023-12-03_14-02-51: Epoch: 11 | Train loss: 0.7271005874751387 | Train acc: {'40X': 78.41, '100X': 80.4, '200X': 83.64, '400X': 84.52, 'avg_acc': 81.74, 'all_acc': 81.67} | Valid loss: 0.9021900087594986 | Valid acc: {'40X': 82.21, '100X': 82.25, '200X': 85.32, '400X': 85.71, 'avg_acc': 83.87, 'all_acc': 83.82}| Runtime: 1.0 mins
2023-12-03_14-02-51: Train summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  68.20  70.33  73.43  72.45    71.10    71.07
1      2  72.40  73.60  81.21  77.34    76.14    76.10
2      3  73.85  76.14  80.13  79.08    77.30    77.26
3      4  77.28  76.10  82.52  83.10    79.75    79.65
4      5  77.94  76.71  80.94  80.95    79.13    79.08
5      6  79.11  78.17  83.15  82.17    80.65    80.60
6      7  81.29  80.24  81.62  84.88    82.01    81.93
7      8  76.99  78.97  83.82  84.04    80.96    80.87
8      9  77.62  80.69  83.49  83.03    81.21    81.17
9     10  78.41  80.40  83.64  84.52    81.74    81.67
2023-12-03_14-02-51: Eval summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  83.96  79.86  83.58  82.42    82.46    82.43
1      2  78.45  79.86  81.59  77.20    79.28    79.33
2      3  81.20  81.77  83.08  81.87    81.98    81.98
3      4  67.17  73.86  76.87  79.95    74.46    74.34
4      5  83.21  80.82  81.84  83.52    82.35    82.30
5      6  81.20  81.29  84.58  84.07    82.78    82.74
6      7  83.96  84.65  86.32  85.99    85.23    85.21
7      8  78.70  81.06  82.09  78.85    80.18    80.21
8      9  77.19  75.30  80.85  74.73    77.02    77.05
9     10  82.21  82.25  85.32  85.71    83.87    83.82
2023-12-03_14-02-51: Final test accuracy: {'40X': 85.46, '100X': 79.33, '200X': 89.33, '400X': 87.36, 'avg_acc': 85.37, 'all_acc': 85.27}
