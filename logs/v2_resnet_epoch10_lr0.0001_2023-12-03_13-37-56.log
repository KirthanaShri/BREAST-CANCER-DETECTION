2023-12-03_13-37-56: config: {'n_epochs': 10, 'batch_size': 32, 'h': 224, 'w': 224, 'lr': 0.0001, 'optimizer': 'adam', 'momentum': 0.9, 'device': 'cuda', 'n_gpus': 2, 'kernel_size': 3, 'flatten': False, 'model': 'resnet', 'num_blocks_list': [3, 4, 6, 3], 'is_batchnorm': False}
2023-12-03_13-37-57: conv1.0.weight: torch.Size([64, 3, 3, 3])
2023-12-03_13-37-57: conv2_x.0.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-37-57: conv2_x.0.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-37-57: conv2_x.1.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-37-57: conv2_x.1.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-37-57: conv2_x.2.sequence.0.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-37-57: conv2_x.2.sequence.2.weight: torch.Size([64, 64, 3, 3])
2023-12-03_13-37-57: conv3_x.0.sequence.0.weight: torch.Size([128, 64, 3, 3])
2023-12-03_13-37-57: conv3_x.0.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-37-57: conv3_x.0.shortcut.0.weight: torch.Size([128, 64, 1, 1])
2023-12-03_13-37-57: conv3_x.1.sequence.0.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-37-57: conv3_x.1.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-37-57: conv3_x.2.sequence.0.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-37-57: conv3_x.2.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-37-57: conv3_x.3.sequence.0.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-37-57: conv3_x.3.sequence.2.weight: torch.Size([128, 128, 3, 3])
2023-12-03_13-37-57: conv4_x.0.sequence.0.weight: torch.Size([256, 128, 3, 3])
2023-12-03_13-37-57: conv4_x.0.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-37-57: conv4_x.0.shortcut.0.weight: torch.Size([256, 128, 1, 1])
2023-12-03_13-37-57: conv4_x.1.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-37-57: conv4_x.1.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-37-57: conv4_x.2.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-37-57: conv4_x.2.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-37-57: conv4_x.3.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-37-57: conv4_x.3.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-37-57: conv4_x.4.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-37-57: conv4_x.4.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-37-57: conv4_x.5.sequence.0.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-37-57: conv4_x.5.sequence.2.weight: torch.Size([256, 256, 3, 3])
2023-12-03_13-37-57: conv5_x.0.sequence.0.weight: torch.Size([512, 256, 3, 3])
2023-12-03_13-37-57: conv5_x.0.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-37-57: conv5_x.0.shortcut.0.weight: torch.Size([512, 256, 1, 1])
2023-12-03_13-37-57: conv5_x.1.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-37-57: conv5_x.1.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-37-57: conv5_x.2.sequence.0.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-37-57: conv5_x.2.sequence.2.weight: torch.Size([512, 512, 3, 3])
2023-12-03_13-37-57: fc.weight: torch.Size([2, 512])
2023-12-03_13-37-57: fc.bias: torch.Size([2])
2023-12-03_13-37-57: 
Total parameters: 21,260,994;	Trainable: 21,260,994
2023-12-03_13-39-26: Epoch: 2 | Train loss: 460.5362824781521 | Train acc: {'40X': 66.0, '100X': 67.15, '200X': 71.33, '400X': 71.48, 'avg_acc': 68.99, 'all_acc': 68.92} | Valid loss: 0.4191842895746231 | Valid acc: {'40X': 79.2, '100X': 80.82, '200X': 85.07, '400X': 82.42, 'avg_acc': 81.88, 'all_acc': 81.86}| Runtime: 1.5 mins
2023-12-03_13-40-54: Epoch: 3 | Train loss: 0.5174432985887334 | Train acc: {'40X': 75.23, '100X': 77.67, '200X': 82.34, '400X': 81.1, 'avg_acc': 79.09, 'all_acc': 79.03} | Valid loss: 0.6403650152683258 | Valid acc: {'40X': 79.2, '100X': 79.86, '200X': 83.33, '400X': 79.67, 'avg_acc': 80.52, 'all_acc': 80.53}| Runtime: 1.5 mins
2023-12-03_13-42-23: Epoch: 4 | Train loss: 0.5563267390872981 | Train acc: {'40X': 76.19, '100X': 76.22, '200X': 81.79, '400X': 83.24, 'avg_acc': 79.36, 'all_acc': 79.24} | Valid loss: 0.45636393040418627 | Valid acc: {'40X': 69.92, '100X': 77.7, '200X': 78.61, '400X': 81.32, 'avg_acc': 76.89, 'all_acc': 76.8}| Runtime: 1.5 mins
2023-12-03_13-43-52: Epoch: 5 | Train loss: 0.3667647314031382 | Train acc: {'40X': 80.13, '100X': 82.25, '200X': 85.83, '400X': 86.43, 'avg_acc': 83.66, 'all_acc': 83.59} | Valid loss: 0.36648727804422376 | Valid acc: {'40X': 81.7, '100X': 82.01, '200X': 85.32, '400X': 85.16, 'avg_acc': 83.55, 'all_acc': 83.5}| Runtime: 1.5 mins
2023-12-03_13-45-21: Epoch: 6 | Train loss: 0.4407776224653463 | Train acc: {'40X': 79.25, '100X': 81.06, '200X': 85.68, '400X': 84.82, 'avg_acc': 82.7, 'all_acc': 82.64} | Valid loss: 0.4274536642432213 | Valid acc: {'40X': 77.94, '100X': 80.58, '200X': 83.83, '400X': 84.34, 'avg_acc': 81.67, 'all_acc': 81.61}| Runtime: 1.5 mins
2023-12-03_13-46-50: Epoch: 7 | Train loss: 0.4036296951408322 | Train acc: {'40X': 80.69, '100X': 82.25, '200X': 85.57, '400X': 85.67, 'avg_acc': 83.54, 'all_acc': 83.49} | Valid loss: 0.37006820619106295 | Valid acc: {'40X': 80.95, '100X': 84.41, '200X': 87.31, '400X': 87.36, 'avg_acc': 85.01, 'all_acc': 84.96}| Runtime: 1.5 mins
2023-12-03_13-48-20: Epoch: 8 | Train loss: 0.33601631083198497 | Train acc: {'40X': 81.99, '100X': 84.57, '200X': 86.5, '400X': 87.35, 'avg_acc': 85.1, 'all_acc': 85.05} | Valid loss: 0.3616092985868454 | Valid acc: {'40X': 83.71, '100X': 82.97, '200X': 87.06, '400X': 85.16, 'avg_acc': 84.72, 'all_acc': 84.7}| Runtime: 1.5 mins
2023-12-03_13-49-49: Epoch: 9 | Train loss: 0.3683872339189858 | Train acc: {'40X': 81.16, '100X': 83.31, '200X': 87.31, '400X': 87.34, 'avg_acc': 84.78, 'all_acc': 84.71} | Valid loss: 0.3803171855211258 | Valid acc: {'40X': 82.96, '100X': 82.01, '200X': 86.82, '400X': 85.71, 'avg_acc': 84.38, 'all_acc': 84.32}| Runtime: 1.5 mins
2023-12-03_13-51-17: Epoch: 10 | Train loss: 0.2988003291491721 | Train acc: {'40X': 84.03, '100X': 87.55, '200X': 87.74, '400X': 89.25, 'avg_acc': 87.14, 'all_acc': 87.1} | Valid loss: 0.41740973979234697 | Valid acc: {'40X': 81.95, '100X': 81.53, '200X': 86.07, '400X': 84.07, 'avg_acc': 83.4, 'all_acc': 83.38}| Runtime: 1.5 mins
2023-12-03_13-52-46: Epoch: 11 | Train loss: 0.2940401904083587 | Train acc: {'40X': 84.52, '100X': 85.39, '200X': 89.63, '400X': 89.63, 'avg_acc': 87.29, 'all_acc': 87.23} | Valid loss: 0.34017021775245665 | Valid acc: {'40X': 84.96, '100X': 83.93, '200X': 87.31, '400X': 87.91, 'avg_acc': 86.03, 'all_acc': 85.97}| Runtime: 1.5 mins
2023-12-03_13-52-46: Train summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  66.00  67.15  71.33  71.48    68.99    68.92
1      2  75.23  77.67  82.34  81.10    79.09    79.03
2      3  76.19  76.22  81.79  83.24    79.36    79.24
3      4  80.13  82.25  85.83  86.43    83.66    83.59
4      5  79.25  81.06  85.68  84.82    82.70    82.64
5      6  80.69  82.25  85.57  85.67    83.54    83.49
6      7  81.99  84.57  86.50  87.35    85.10    85.05
7      8  81.16  83.31  87.31  87.34    84.78    84.71
8      9  84.03  87.55  87.74  89.25    87.14    87.10
9     10  84.52  85.39  89.63  89.63    87.29    87.23
2023-12-03_13-52-46: Eval summary:    epoch    40X   100X   200X   400X  avg_acc  all_acc
0      1  79.20  80.82  85.07  82.42    81.88    81.86
1      2  79.20  79.86  83.33  79.67    80.52    80.53
2      3  69.92  77.70  78.61  81.32    76.89    76.80
3      4  81.70  82.01  85.32  85.16    83.55    83.50
4      5  77.94  80.58  83.83  84.34    81.67    81.61
5      6  80.95  84.41  87.31  87.36    85.01    84.96
6      7  83.71  82.97  87.06  85.16    84.72    84.70
7      8  82.96  82.01  86.82  85.71    84.38    84.32
8      9  81.95  81.53  86.07  84.07    83.40    83.38
9     10  84.96  83.93  87.31  87.91    86.03    85.97
2023-12-03_13-52-46: Final test accuracy: {'40X': 83.71, '100X': 81.25, '200X': 89.58, '400X': 88.19, 'avg_acc': 85.68, 'all_acc': 85.59}
